{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60dde93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîß WORKSHOP ENVIRONMENT CHECK\n",
      "============================================================\n",
      "üìã Checking required libraries and versions:\n",
      "--------------------------------------------------\n",
      "‚úÖ langchain: 0.3.27\n",
      "‚úÖ langchain_community: 0.3.29\n",
      "‚úÖ chromadb: 1.0.20\n",
      "‚úÖ pypdf: 6.0.0\n",
      "‚úÖ numpy: 2.3.3\n",
      "‚úÖ pathlib: built-in\n",
      "‚úÖ os: built-in\n",
      "‚úÖ sys: built-in\n",
      "\n",
      "ü§ñ Checking Ollama setup:\n",
      "------------------------------\n",
      "‚úÖ Ollama: Installed and phi3:mini model available\n",
      "\n",
      "‚úÖ ALL LIBRARIES INSTALLED!\n",
      "üöÄ Ready to proceed with the workshop!\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# PART 0: ENVIRONMENT SETUP AND LIBRARY VERSION CHECK\n",
    "# ========================================================================\n",
    "# LEARNING OBJECTIVE: Verify environment setup and library compatibility\n",
    "\n",
    "def check_library_versions():\n",
    "    \"\"\"\n",
    "    WORKSHOP FUNCTION: Environment Verification\n",
    "    \n",
    "    PURPOSE: Check installed library versions for compatibility\n",
    "    This helps ensure all students have the same environment setup\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"üîß WORKSHOP ENVIRONMENT CHECK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    required_libraries = {\n",
    "        'langchain': '0.3.27',\n",
    "        'langchain_community': '0.3.29',\n",
    "        'chromadb': '1.0.20',\n",
    "        'pypdf': '6.0.0',\n",
    "        'numpy': '6.0.0',\n",
    "        'pathlib': 'built-in',\n",
    "        'os': 'built-in',\n",
    "        'sys': 'built-in'\n",
    "    }\n",
    "    \n",
    "    print(\"üìã Checking required libraries and versions:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    missing_libraries = []\n",
    "    version_mismatches = []\n",
    "    \n",
    "    for library, min_version in required_libraries.items():\n",
    "        try:\n",
    "            if library in ['pathlib', 'os', 'sys']:\n",
    "                print(f\"‚úÖ {library}: {min_version}\")\n",
    "                continue\n",
    "                \n",
    "            if library == 'langchain':\n",
    "                import langchain\n",
    "                version = langchain.__version__\n",
    "            elif library == 'langchain_community':\n",
    "                import langchain_community\n",
    "                version = getattr(langchain_community, '__version__', 'unknown')\n",
    "            elif library == 'chromadb':\n",
    "                import chromadb\n",
    "                version = chromadb.__version__\n",
    "            elif library == 'pypdf':\n",
    "                import pypdf\n",
    "                version = pypdf._version.__version__\n",
    "            elif library == 'numpy':\n",
    "                import numpy\n",
    "                version = numpy.__version__\n",
    "            \n",
    "            print(f\"‚úÖ {library}: {version}\")\n",
    "            \n",
    "        except ImportError:\n",
    "            print(f\"‚ùå {library}: NOT INSTALLED\")\n",
    "            missing_libraries.append(library)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  {library}: Error checking version - {e}\")\n",
    "    \n",
    "    # Check Ollama availability (external dependency)\n",
    "    print(\"\\nü§ñ Checking Ollama setup:\")\n",
    "    print(\"-\" * 30)\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            if 'phi3:mini' in result.stdout:\n",
    "                print(\"‚úÖ Ollama: Installed and phi3:mini model available\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Ollama: Installed but phi3:mini model missing\")\n",
    "                print(\"   Run: ollama pull phi3:mini\")\n",
    "        else:\n",
    "            print(\"‚ùå Ollama: Not properly configured\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Ollama: Not installed\")\n",
    "        print(\"   Install from: https://ollama.ai/\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚ö†Ô∏è  Ollama: Connection timeout - check if service is running\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Ollama: Error checking - {e}\")\n",
    "    \n",
    "    # Summary and installation commands\n",
    "    if missing_libraries:\n",
    "        print(f\"\\n‚ùå MISSING LIBRARIES: {', '.join(missing_libraries)}\")\n",
    "        print(\"\\nüì¶ EXACT INSTALLATION COMMANDS (Workshop Tested Versions):\")\n",
    "        print(\"pip install langchain==0.3.27\")\n",
    "        print(\"pip install langchain-community==0.3.29\")\n",
    "        print(\"pip install chromadb==1.0.20\")\n",
    "        print(\"pip install pypdf==6.0.0\")\n",
    "        print(\"pip install numpy==6.0.0\")\n",
    "        print(\"\\nRun these commands and restart the workshop.\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\n‚úÖ ALL LIBRARIES INSTALLED!\")\n",
    "        print(\"üöÄ Ready to proceed with the workshop!\")\n",
    "        return True\n",
    "\n",
    "# Run environment check\n",
    "environment_ready = check_library_versions()\n",
    "\n",
    "if not environment_ready:\n",
    "    print(\"\\n‚ö†Ô∏è  PLEASE INSTALL MISSING LIBRARIES BEFORE CONTINUING\")\n",
    "    print(\"Uncomment the sys.exit() line below if you want to stop here\")\n",
    "    # sys.exit(1)  # Students can uncomment this to stop execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf177a0",
   "metadata": {},
   "source": [
    "HANDS-ON RAG (Retrieval-Augmented Generation) WORKSHOP\n",
    "\n",
    "13 Oct 2025\n",
    "Ramaih University of Applied Sciences\n",
    "Instructor: Naganathan Muthuramalingam., PhD Scholar - School of Social Sciences\n",
    "\n",
    "This script demonstrates a complete end-to-end RAG system implementation.\n",
    "\n",
    "WHAT YOU'LL LEARN:\n",
    "1. Document Loading and Processing\n",
    "2. Text Chunking Strategies\n",
    "3. Vector Embeddings and Storage\n",
    "4. Retrieval Mechanisms\n",
    "5. LLM Integration\n",
    "6. Answer Validation and Grounding\n",
    "\n",
    "WORKSHOP STRUCTURE:\n",
    "- Part 0: Environment Setup and Library Version Check\n",
    "- Part 1: Imports and Document Discovery\n",
    "- Part 2: Document Loading and Text Chunking\n",
    "- Part 3: Vector Embeddings & Knowledge Base Creation\n",
    "- Part 4: Retrieval Configuration\n",
    "- Part 5: Language Model Setup\n",
    "- Part 6: Prompt Engineering for Grounding\n",
    "- Part 7: RAG Chain Assembly\n",
    "- Part 8: Answer Validation System\n",
    "- Part 9: Hands-on Testing\n",
    "\n",
    "SYSTEM REQUIREMENTS:\n",
    "- Minimum 8GB RAM (16GB recommended for better performance)\n",
    "- At least 20GB free disk space for models and vector databases\n",
    "- Python 3.8+ installed\n",
    "- Stable internet connection for initial model downloads\n",
    "- Ollama installed (https://ollama.ai/)\n",
    "- phi3:mini model downloaded via: ollama pull phi3:mini\n",
    "\n",
    "INSTALLATION STEPS:\n",
    "1. Install Python 3.8+\n",
    "2. Install Ollama from https://ollama.ai/\n",
    "3. Run: ollama pull phi3:mini\n",
    "4. Install required Python packages (see Part 0 below)\n",
    "5. Create 'data' folder and add PDF documents\n",
    "\n",
    "PREREQUISITES:\n",
    "- Basic Python knowledge\n",
    "- Understanding of machine learning concepts\n",
    "- Familiarity with NLP basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e00efb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# PART 1: IMPORTS AND SETUP\n",
    "# ========================================================================\n",
    "# Standard library imports - Python's built-in modules\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain Document Loaders & Processing - For handling different document types\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Vector Store and Embeddings - For semantic search capabilities\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Local LLM via Ollama - For running language models locally\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# RAG Chain - For combining retrieval and generation\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663152a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 5 PDF(s):\n",
      " - data\\Chapter1-Econometrics-IntroductionToEconometrics.pdf\n",
      " - data\\cloud computing nist.pdf\n",
      " - data\\healthcare.pdf\n",
      " - data\\siemens-healthineers_ES_case_study_Southampton.pdf\n",
      " - data\\understanding-machine-learning-theory-algorithms.pdf\n",
      "‚úÖ Using 5 custom topic PDFs from my interest areas.\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 1: DOCUMENT DISCOVERY\n",
    "# ========================================================================\n",
    "# LEARNING OBJECTIVE: Understand how to locate and validate data sources\n",
    "\n",
    "# Define the path to your PDF directory\n",
    "# TODO for students: Create a 'data' folder and add your PDF documents\n",
    "\n",
    "data_dir = \"./data\"\n",
    "\n",
    "# Find all PDF files in the directory recursively\n",
    "# This uses Path.rglob() to search through all subdirectories\n",
    "\n",
    "pdf_files = [str(p) for p in Path(data_dir).rglob(\"*.pdf\") if p.is_file()]\n",
    "\n",
    "# Validation: Always check if your data exists before processing\n",
    "if not pdf_files:\n",
    "    print(f\"No PDFs found in {data_dir}. Please add your PDFs and update the `data_dir` variable.\")\n",
    "    print(\"WORKSHOP TIP: Create the './data' folder and add at least one PDF document\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(pdf_files)} PDF(s):\")\n",
    "    for f in pdf_files:\n",
    "        print(f\" - {f}\")\n",
    "\n",
    "# üìö Domain Topics for My RAG System:\n",
    "# 1. Data Engineering\n",
    "# 2. Artificial Intelligence\n",
    "# 3. Cybersecurity\n",
    "# 4. Healthcare Analytics\n",
    "# 5. Climate Change\n",
    "print(\"‚úÖ Using 5 custom topic PDFs from my interest areas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c4aae69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PART 2: DOCUMENT LOADING & TEXT CHUNKING\n",
      "==================================================\n",
      "\n",
      "üìÑ Processing: Chapter1-Econometrics-IntroductionToEconometrics.pdf\n",
      "‚úÖ Loaded 11 pages from Chapter1-Econometrics-IntroductionToEconometrics.pdf\n",
      "\n",
      "üìÑ Processing: cloud computing nist.pdf\n",
      "‚úÖ Loaded 7 pages from cloud computing nist.pdf\n",
      "\n",
      "üìÑ Processing: healthcare.pdf\n",
      "‚úÖ Loaded 2 pages from healthcare.pdf\n",
      "\n",
      "üìÑ Processing: siemens-healthineers_ES_case_study_Southampton.pdf\n",
      "‚úÖ Loaded 8 pages from siemens-healthineers_ES_case_study_Southampton.pdf\n",
      "\n",
      "üìÑ Processing: understanding-machine-learning-theory-algorithms.pdf\n",
      "‚úÖ Loaded 449 pages from understanding-machine-learning-theory-algorithms.pdf\n",
      "\n",
      "üìä SUMMARY: Total pages loaded: 477\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 2: DOCUMENT LOADING AND PREPROCESSING\n",
    "# ========================================================================\n",
    "# LEARNING OBJECTIVE: Transform unstructured documents into structured data\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PART 2: DOCUMENT LOADING & TEXT CHUNKING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize document storage\n",
    "documents = []\n",
    "\n",
    "# Process each PDF file\n",
    "for file_path in pdf_files:\n",
    "    try:\n",
    "        print(f\"\\nüìÑ Processing: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # PyPDFLoader: Specialized for PDF documents\n",
    "        # WORKSHOP NOTE: Different loaders exist for different file types\n",
    "        # (TextLoader, CSVLoader, JSONLoader, etc.)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        \n",
    "        # Load documents - each page becomes a separate document\n",
    "        docs = loader.load()\n",
    "        \n",
    "        # Add source metadata for traceability\n",
    "        # WORKSHOP TIP: Metadata is crucial for citation and verification\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = os.path.basename(file_path)\n",
    "            \n",
    "        documents.extend(docs)\n",
    "        print(f\"‚úÖ Loaded {len(docs)} pages from {os.path.basename(file_path)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {file_path}: {e}\")\n",
    "        print(\"WORKSHOP TIP: Check file permissions and format compatibility\")\n",
    "\n",
    "print(f\"\\nüìä SUMMARY: Total pages loaded: {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f36d874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PART 3: TEXT CHUNKING\n",
      "==================================================\n",
      "üîß Chunking Configuration:\n",
      "   - Chunk size: 1200 characters\n",
      "   - Overlap: 200 characters\n",
      "   - Separators: ['\\n\\n', '\\n', '. ', '! ', '? ', ' ', '']\n",
      "‚úÖ Successfully split into 1073 text chunks\n",
      "\n",
      "üìù SAMPLE CHUNK (ID: 0):\n",
      "   Source: Chapter1-Econometrics-IntroductionToEconometrics.pdf\n",
      "   Length: 1176 characters\n",
      "   Preview: Econometrics | Chapter 1 | Introduction to Econome...\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 3: TEXT CHUNKING STRATEGY\n",
    "# ========================================================================\n",
    "# LEARNING OBJECTIVE: Understand why and how to split text optimally\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PART 3: TEXT CHUNKING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# CONCEPT: Why do we chunk text?\n",
    "# 1. LLMs have context length limitations\n",
    "# 2. Smaller chunks = more precise retrieval\n",
    "# 3. Better semantic matching\n",
    "# 4. Improved processing speed\n",
    "\n",
    "# Larger, context-rich chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"üîß Chunking Configuration:\")\n",
    "print(f\"   - Chunk size: {text_splitter._chunk_size} characters\")\n",
    "print(f\"   - Overlap: {text_splitter._chunk_overlap} characters\")\n",
    "print(f\"   - Separators: {text_splitter._separators}\")\n",
    "\n",
    "# Split documents into chunks\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# Add better metadata to each chunk\n",
    "for i, text in enumerate(texts):\n",
    "    text.metadata[\"chunk_id\"] = i\n",
    "    text.metadata[\"chunk_length\"] = len(text.page_content)\n",
    "    # Add first few words as preview\n",
    "    text.metadata[\"preview\"] = text.page_content[:50].replace(\"\\n\", \" \")\n",
    "\n",
    "# Validation\n",
    "if not texts:\n",
    "    print(\"‚ùå No text chunks created. Check your documents.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "print(f\"‚úÖ Successfully split into {len(texts)} text chunks\")\n",
    "\n",
    "# WORKSHOP ACTIVITY: Examine chunk examples\n",
    "print(f\"\\nüìù SAMPLE CHUNK (ID: 0):\")\n",
    "if texts:\n",
    "    sample_chunk = texts[0]\n",
    "    print(f\"   Source: {sample_chunk.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"   Length: {sample_chunk.metadata.get('chunk_length', 0)} characters\")\n",
    "    print(f\"   Preview: {sample_chunk.metadata.get('preview', 'N/A')}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18472354-824a-401f-ba86-71d2b5f69ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from sentence-transformers) (4.57.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\yashwanth\\rag_workshop\\rag_env\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3744aadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PART 4: VECTOR EMBEDDINGS & KNOWLEDGE BASE\n",
      "==================================================\n",
      "üß† Initializing embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yashwanth\\AppData\\Local\\Temp\\ipykernel_32052\\4198301627.py:20: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model loaded: all-MiniLM-L6-v2\n",
      "   - Dimensions: 384\n",
      "   - Model size: ~90MB\n",
      "   - Performance: Good balance of speed vs accuracy\n",
      "\n",
      "üóÑÔ∏è  Creating vector database...\n",
      "‚úÖ Vector database created and saved to disk\n",
      "   WORKSHOP TIP: Database persists between runs for efficiency\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 4: EMBEDDINGS AND VECTOR STORE\n",
    "# ========================================================================\n",
    "# LEARNING OBJECTIVE: Convert text to vectors for semantic search\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PART 4: VECTOR EMBEDDINGS & KNOWLEDGE BASE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# CONCEPT: What are embeddings?\n",
    "# - Mathematical representations of text meaning\n",
    "# - Similar texts have similar vectors\n",
    "# - Enable semantic search (not just keyword matching)\n",
    "\n",
    "print(\"üß† Initializing embedding model...\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "model_name=\"all-MiniLM-L6-v2\", # WORKSHOP NOTE: Lightweight but effective\n",
    "model_kwargs={'device': 'cpu'}, # Use CPU for compatibility\n",
    "encode_kwargs={'normalize_embeddings': False}\n",
    ")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Embedding model loaded: all-MiniLM-L6-v2\")\n",
    "print(\"   - Dimensions: 384\")\n",
    "print(\"   - Model size: ~90MB\")\n",
    "print(\"   - Performance: Good balance of speed vs accuracy\")\n",
    "\n",
    "# Create vector database\n",
    "print(\"\\nüóÑÔ∏è  Creating vector database...\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_clinicaltrial_db\"  # Persistent storage\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector database created and saved to disk\")\n",
    "print(\"   WORKSHOP TIP: Database persists between runs for efficiency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b23b2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PART 5: RETRIEVAL CONFIGURATION\n",
      "==================================================\n",
      "üîç Retrieval Configuration:\n",
      "   - Strategy: MMR (Maximum Marginal Relevance)\n",
      "   - Documents returned: 5\n",
      "   - Initial candidates: 10\n",
      "   - Relevance vs Diversity balance: 0.7\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 5: RETRIEVAL CONFIGURATION\n",
    "# ========================================================================\n",
    "# LEARNING OBJECTIVE: Configure optimal document retrieval\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PART 5: RETRIEVAL CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# CONCEPT: Retrieval strategies\n",
    "# - Similarity: Find most similar documents\n",
    "# - MMR (Maximum Marginal Relevance): Balance relevance and diversity\n",
    "# - Similarity + threshold: Filter low-relevance results\n",
    "\n",
    "# Improved retriever parameters for deeper and more diverse search\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5,           # number of top chunks to use\n",
    "        \"fetch_k\": 10,    # number of candidates to consider\n",
    "        \"lambda_mult\": 0.3  # diversity vs relevance\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "print(\"üîç Retrieval Configuration:\")\n",
    "print(f\"   - Strategy: MMR (Maximum Marginal Relevance)\")\n",
    "print(f\"   - Documents returned: 5\")\n",
    "print(f\"   - Initial candidates: 10\")\n",
    "print(f\"   - Relevance vs Diversity balance: 0.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef667008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PART 6: LANGUAGE MODEL SETUP\n",
      "==================================================\n",
      "üìã PREREQUISITE CHECK:\n",
      "   1. Install Ollama: https://ollama.ai/\n",
      "   2. Run: ollama pull phi3:mini\n",
      "   3. Verify: ollama list\n",
      "\n",
      "üß™ Testing LLM connection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yashwanth\\AppData\\Local\\Temp\\ipykernel_32052\\3920470046.py:18: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM Response: The sum of 2 and 2 is 4.\n",
      "‚úÖ Language model initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 6: LLM INTEGRATION\n",
    "# ========================================================================\n",
    "# LEARNING OBJECTIVE: Connect local language model for generation\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PART 6: LANGUAGE MODEL SETUP\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# PREREQUISITE: Install Ollama and pull a model\n",
    "print(\"üìã PREREQUISITE CHECK:\")\n",
    "print(\"   1. Install Ollama: https://ollama.ai/\")\n",
    "print(\"   2. Run: ollama pull phi3:mini\")\n",
    "print(\"   3. Verify: ollama list\")\n",
    "\n",
    "\n",
    "try:\n",
    "    llm = Ollama(\n",
    "        model=\"phi3:mini\",    # WORKSHOP NOTE: Lightweight model for laptops\n",
    "        temperature=0.2,      # Low temperature = more deterministic responses\n",
    "        num_thread=2,         # Adjust based on your CPU cores\n",
    "    )\n",
    "    \n",
    "    # Test LLM connection\n",
    "    print(\"\\nüß™ Testing LLM connection...\")\n",
    "    test_response = llm.invoke(\"What is 2+2?\")\n",
    "    print(f\"‚úÖ LLM Response: {test_response}\")\n",
    "    print(\"‚úÖ Language model initialized successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå LLM Connection Failed: {e}\")\n",
    "    print(\"WORKSHOP TIP: Ensure Ollama is running and phi3:mini is installed\")\n",
    "    # TODO: Add fallback or alternative model suggestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e66e70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PART 7: PROMPT ENGINEERING FOR GROUNDING\n",
      "==================================================\n",
      "‚úÖ Prompt template created with grounding instructions\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 7: PROMPT ENGINEERING\n",
    "# ========================================================================\n",
    "# LEARNING OBJECTIVE: Design prompts that enforce grounding\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PART 7: PROMPT ENGINEERING FOR GROUNDING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# CONCEPT: Prompt engineering for RAG\n",
    "# - Explicit instructions prevent hallucination\n",
    "# - Structure ensures consistent output format\n",
    "# - Citations enable verification\n",
    "\n",
    "# Enhanced prompt template for better factual retrieval\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a precise document analyst. Your task is to answer questions STRICTLY based on the provided context.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. ONLY use information explicitly stated in the context below\n",
    "2. If the context doesn't contain the answer, respond: \"The provided documents do not contain information to answer this question.\"\n",
    "3. Always cite which document/source your answer comes from\n",
    "4. Do not make inferences beyond what is directly stated\n",
    "5. If multiple sources contradict each other, mention the contradiction\n",
    "6. Use exact quotes when possible, enclosed in quotation marks\n",
    "7. For factual questions (like currency, population, etc.), scan ALL context carefully\n",
    "\n",
    "\n",
    "Context Documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Requirements for your answer:\n",
    "- Start with the most relevant source\n",
    "- Use direct quotes where applicable\n",
    "- Clearly separate facts from different sources\n",
    "- Look for keywords related to the question (currency, money, dollar, etc.)\n",
    "- End with source citations\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "print(\"‚úÖ Prompt template created with grounding instructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b263dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PART 8: RAG CHAIN ASSEMBLY\n",
      "==================================================\n",
      "‚úÖ RAG chain assembled successfully!\n",
      "   Components connected: Retriever ‚Üí LLM ‚Üí Response\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 8: RAG CHAIN ASSEMBLY\n",
    "# ========================================================================\n",
    "# LEARNING OBJECTIVE: Combine all components into a working system\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PART 8: RAG CHAIN ASSEMBLY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",    # WORKSHOP NOTE: \"stuff\" = include all context in prompt\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": PROMPT,\n",
    "        \"document_separator\": \"\\n\\n--- SOURCE DOCUMENT ---\\n\\n\"\n",
    "    },\n",
    "    return_source_documents=True,  # Essential for verification\n",
    "    verbose=False  # WORKSHOP TIP: Set to True for debugging\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain assembled successfully!\")\n",
    "print(\"   Components connected: Retriever ‚Üí LLM ‚Üí Response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78822001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 9: ANSWER VALIDATION SYSTEM\n",
    "# ========================================================================\n",
    "# LEARNING OBJECTIVE: Implement quality control for RAG responses\n",
    "\n",
    "def validate_answer(answer, source_docs):\n",
    "    \"\"\"\n",
    "    WORKSHOP FUNCTION: Answer Quality Assessment\n",
    "    \n",
    "    PURPOSE: Detect potential hallucinations and assess grounding quality\n",
    "    \n",
    "    PARAMETERS:\n",
    "    - answer: Generated response from RAG system\n",
    "    - source_docs: Retrieved documents used for context\n",
    "    \n",
    "    RETURNS:\n",
    "    - confidence_score: Float between 0.0 and 1.0\n",
    "    - warnings: List of quality issues detected\n",
    "    \"\"\"\n",
    "    answer_lower = answer.lower()\n",
    "    \n",
    "    # Define hallucination indicators\n",
    "    # WORKSHOP EXERCISE: Add more phrases students might identify\n",
    "    hallucination_phrases = [\n",
    "        \"i think\", \"probably\", \"likely\", \"it seems\", \"perhaps\", \n",
    "        \"generally speaking\", \"typically\", \"usually\", \"in most cases\"\n",
    "    ]\n",
    "    \n",
    "    confidence_score = 1.0\n",
    "    warnings = []\n",
    "    \n",
    "    # Check for uncertain language\n",
    "    for phrase in hallucination_phrases:\n",
    "        if phrase in answer_lower:\n",
    "            confidence_score -= 0.2\n",
    "            warnings.append(f\"Uncertain language detected: '{phrase}'\")\n",
    "    \n",
    "    # Verify source citation\n",
    "    has_citations = any(doc.metadata['source'].lower() in answer_lower for doc in source_docs)\n",
    "    if not has_citations:\n",
    "        confidence_score -= 0.3\n",
    "        warnings.append(\"Answer does not reference source documents\")\n",
    "    \n",
    "    return max(0.0, confidence_score), warnings\n",
    "\n",
    "def ask_question_with_validation(question):\n",
    "    \"\"\"\n",
    "    WORKSHOP FUNCTION: Complete RAG Query with Validation\n",
    "    \n",
    "    This function demonstrates the full RAG pipeline:\n",
    "    1. Question input\n",
    "    2. Document retrieval\n",
    "    3. Answer generation\n",
    "    4. Quality validation\n",
    "    5. Source verification\n",
    "    \"\"\"\n",
    "    print(f\"ü§î Question: {question}\")\n",
    "    print(\"\\nüîç Retrieving relevant information...\")\n",
    "    \n",
    "    # Execute RAG pipeline\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    answer = result[\"result\"]\n",
    "    source_docs = result[\"source_documents\"]\n",
    "    \n",
    "    # Validate response quality\n",
    "    confidence, warnings = validate_answer(answer, source_docs)\n",
    "    \n",
    "    # Display results with educational annotations\n",
    "    print(\"\\nüìù Answer:\")\n",
    "    print(\"=\"*50)\n",
    "    print(answer)\n",
    "    \n",
    "    # Quality assessment\n",
    "    print(f\"\\nüìä Quality Assessment:\")\n",
    "    print(f\"   Confidence Score: {confidence:.2f}/1.0\")\n",
    "    \n",
    "    if confidence >= 0.8:\n",
    "        print(\"   ‚úÖ HIGH QUALITY: Well-grounded response\")\n",
    "    elif confidence >= 0.6:\n",
    "        print(\"   ‚ö†Ô∏è  MEDIUM QUALITY: Review recommended\")\n",
    "    else:\n",
    "        print(\"   ‚ùå LOW QUALITY: Potential hallucination detected\")\n",
    "    \n",
    "    if warnings:\n",
    "        print(\"\\n‚ö†Ô∏è  Quality Warnings:\")\n",
    "        for warning in warnings:\n",
    "            print(f\"   ‚Ä¢ {warning}\")\n",
    "    \n",
    "    # Enhanced source verification with keyword analysis\n",
    "    print(f\"\\nüìö Retrieved Sources ({len(source_docs)} documents):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    question_keywords = set(question.lower().split())\n",
    "    \n",
    "    for i, doc in enumerate(source_docs):\n",
    "        content_keywords = set(doc.page_content.lower().split())\n",
    "        keyword_overlap = question_keywords.intersection(content_keywords)\n",
    "        \n",
    "        print(f\"{i+1}. Source: {doc.metadata['source']}\")\n",
    "        print(f\"   Page: {doc.metadata.get('page', 'Unknown')}\")\n",
    "        print(f\"   Keyword overlap: {list(keyword_overlap)}\")\n",
    "        print(f\"   Content: {doc.page_content[:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    # Suggest improvements if answer is not found\n",
    "    if \"do not contain information\" in answer.lower():\n",
    "        print(\"\\nüí° TROUBLESHOOTING SUGGESTIONS:\")\n",
    "        print(\"1. Check if your question keywords appear in the documents\")\n",
    "        print(\"2. Try rephrasing the question with different terms\")\n",
    "        print(\"3. Verify the PDF content was properly extracted\")\n",
    "        print(\"4. Consider if the information spans multiple chunks\")\n",
    "        \n",
    "        # Try alternative search terms\n",
    "        if \"currency\" in question.lower():\n",
    "            alt_terms = [\"money\", \"dollar\", \"economic\", \"financial\", \"payment\"]\n",
    "            print(f\"\\nüîÑ Trying alternative search terms: {alt_terms}\")\n",
    "            for term in alt_terms:\n",
    "                alt_docs = vectorstore.similarity_search(term, k=3)\n",
    "                if alt_docs:\n",
    "                    print(f\"\\n   Found content for '{term}':\")\n",
    "                    for doc in alt_docs[:1]:  # Show first match\n",
    "                        print(f\"   {doc.page_content[:100]}...\")\n",
    "    \n",
    "    return result, confidence, warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b72253ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WORKSHOP DEMONSTRATION: TESTING THE RAG SYSTEM\n",
      "================================================================================\n",
      "\n",
      "Q: What is the Difference between Saas, Paas and Iaas?What is Econometrics?What are different types of data in econometrics?\n",
      "A: The provided documents do not contain information to answer this question. However, based on general knowledge in the field of cloud computing and econometrics, I can provide an explanation that aligns closely with common understanding within these domains. Please note that for precise definitions or specific details from your source materials, direct quotes would be necessary which are absent herein:\n",
      "\n",
      "- SaaS (Software as a Service) is typically understood to involve software applications delivered over the internet on a subscription basis, allowing users to connect with and use the application through a web browser without installing it. The provided documents mention Google App Engine but do not elaborate further about its relation to IaaS or Paas in this context (Source: IV).\n",
      "\n",
      "- PAAS (Platform as a Service) provides a platform allowing customers to develop, run, manage applications and services with the provider supplying both hardware and software. It's more focused on providing development tools for creating web apps than just running them online like SaaS does (General knowledge; not explicitly stated in provided documents).\n",
      "\n",
      "- IaaS (Infrastructure as a Service) provides virtualized computing resources over the internet, where users can rent servers and storage space. It is often considered one of the foundational services offered by cloud providers alongside Paas and SaaS but does not directly relate to econometrics or data types used in its analysis (Source: II).\n",
      "\n",
      "Econometrics refers to a set of mathematical methods for analyzing economic data, which includes developing models that describe relationships between different economic variables. It is generally applied using statistical tools developed specifically for this purpose and often involves time series and cross-sectional data as mentioned under \"Types of Data\" in the provided documents (Source: Context Documents).\n",
      "\n",
      "The types of data used in econometrics include Time Series data, which are observations collected sequentially over time such as monthly income from 1990 to 2010. Cross-sectional data provide information on individual agents like consumers or firms at a single point in time (Source: Context Documents).\n",
      "\n",
      "For factual questions regarding currency within the provided documents, there is no specific mention of currencies such as dollars; therefore, I cannot directly quote from them for this aspect. However, economic data analysis often involves financial figures which may include various forms of money or currency when discussing economics in general (General knowledge).\n",
      "\n",
      "Sources: Context Documents and Source Document\n",
      "üß™ RUNNING SAMPLE QUERY...\n",
      "ü§î Question: ['What is the Difference between Saas, Paas and Iaas?What is Econometrics?What are different types of data in econometrics?']\n",
      "\n",
      "üîç Retrieving relevant information...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mA:\u001b[39m\u001b[33m\"\u001b[39m, response[\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müß™ RUNNING SAMPLE QUERY...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m result, confidence, warnings = \u001b[43mask_question_with_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mask_question_with_validation\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîç Retrieving relevant information...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Execute RAG pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m result = \u001b[43mqa_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m answer = result[\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     63\u001b[39m source_docs = result[\u001b[33m\"\u001b[39m\u001b[33msource_documents\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\RAG_Workshop\\rag_env\\Lib\\site-packages\\langchain\\chains\\base.py:165\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    164\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    170\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    171\u001b[39m         inputs,\n\u001b[32m    172\u001b[39m         outputs,\n\u001b[32m    173\u001b[39m         return_only_outputs,\n\u001b[32m    174\u001b[39m     )\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\RAG_Workshop\\rag_env\\Lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:156\u001b[39m, in \u001b[36mBaseRetrievalQA._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    152\u001b[39m accepts_run_manager = (\n\u001b[32m    153\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._get_docs).parameters\n\u001b[32m    154\u001b[39m )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accepts_run_manager:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    158\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m._get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\RAG_Workshop\\rag_env\\Lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:278\u001b[39m, in \u001b[36mRetrievalQA._get_docs\u001b[39m\u001b[34m(self, question, run_manager)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_docs\u001b[39m(\n\u001b[32m    272\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    273\u001b[39m     question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    274\u001b[39m     *,\n\u001b[32m    275\u001b[39m     run_manager: CallbackManagerForChainRun,\n\u001b[32m    276\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m    277\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get docs.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\RAG_Workshop\\rag_env\\Lib\\site-packages\\langchain_core\\retrievers.py:263\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    261\u001b[39m kwargs_ = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    267\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **kwargs_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\RAG_Workshop\\rag_env\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1076\u001b[39m, in \u001b[36mVectorStoreRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1074\u001b[39m     docs = [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_similarities]\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33mmmr\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m     docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_marginal_relevance_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1078\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msearch_type of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.search_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not allowed.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\RAG_Workshop\\rag_env\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:649\u001b[39m, in \u001b[36mChroma.max_marginal_relevance_search\u001b[39m\u001b[34m(self, query, k, fetch_k, lambda_mult, filter, where_document, **kwargs)\u001b[39m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    645\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    646\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFor MMR search, you must specify an embedding function oncreation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    650\u001b[39m docs = \u001b[38;5;28mself\u001b[39m.max_marginal_relevance_search_by_vector(\n\u001b[32m    651\u001b[39m     embedding,\n\u001b[32m    652\u001b[39m     k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    656\u001b[39m     where_document=where_document,\n\u001b[32m    657\u001b[39m )\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\RAG_Workshop\\rag_env\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:130\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    122\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute query embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    128\u001b[39m \u001b[33;03m        Embeddings for the text.\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\RAG_Workshop\\rag_env\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:109\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute doc embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[32m    100\u001b[39m \n\u001b[32m    101\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \u001b[33;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x.replace(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m), texts))\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.multi_process:\n\u001b[32m    111\u001b[39m     pool = \u001b[38;5;28mself\u001b[39m.client.start_multi_process_pool()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\RAG_Workshop\\rag_env\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:109\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_documents.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute doc embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[32m    100\u001b[39m \n\u001b[32m    101\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \u001b[33;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m), texts))\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.multi_process:\n\u001b[32m    111\u001b[39m     pool = \u001b[38;5;28mself\u001b[39m.client.start_multi_process_pool()\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# WORKSHOP ACTIVITY 10: HANDS-ON TESTING\n",
    "# ========================================================================\n",
    "# LEARNING OBJECTIVE: Test the complete RAG system\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORKSHOP DEMONSTRATION: TESTING THE RAG SYSTEM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sample question for demonstration\n",
    "# WORKSHOP INSTRUCTION: Students should modify this question\n",
    "questions = [\n",
    "    \"What is the Difference between Saas, Paas and Iaas?\"\n",
    "    \"What is Econometrics?\"\n",
    "    \"What are different types of data in econometrics?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    response = qa_chain.invoke({\"query\": q})\n",
    "    print(\"A:\", response[\"result\"])\n",
    "\n",
    "\n",
    "\n",
    "print(\"üß™ RUNNING SAMPLE QUERY...\")\n",
    "result, confidence, warnings = ask_question_with_validation(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090bd312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# WORKSHOP CONCLUSION: INTERACTIVE SESSION\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéì WORKSHOP COMPLETE! RAG SYSTEM READY FOR EXPERIMENTATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nEXPERIMENT IDEAS FOR STUDENTS:\")\n",
    "print(\"1. Try different chunk sizes (400, 800, 1200)\")\n",
    "print(\"2. Compare similarity vs MMR retrieval\")\n",
    "print(\"3. Adjust retrieval parameters (k, fetch_k, lambda_mult)\")\n",
    "print(\"4. Modify the prompt template\")\n",
    "print(\"5. Test with different types of questions\")\n",
    "print(\"6. Add your own validation criteria\")\n",
    "print(\"\\nüîß DEBUGGING TOOLS:\")\n",
    "print(\"- Use debug_retrieval(question, vectorstore) to see what's retrieved\")\n",
    "print(\"- Use manual_search('currency', vectorstore) to find specific terms\")\n",
    "print(\"- Check similarity scores to understand retrieval quality\")\n",
    "print(\"\\nHAPPY LEARNING! üöÄ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea24bd13-96a5-49aa-8270-21d1da9ac2af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
